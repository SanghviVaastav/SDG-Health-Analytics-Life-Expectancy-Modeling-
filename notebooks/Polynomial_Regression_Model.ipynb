{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Life Expectancy with Polynomial Regression & Lasso Regularization\n",
                "This notebook aims to predict the Life Expectancy of various countries using a variety of health and economic indicators. \n",
                "\n",
                "Because the relationship between some variables (like GDP or BMI) and life expectancy might not be perfectly linear, we're going to generate **Polynomial Features**. Then, to prevent the model from overfitting on all these new features, we'll apply **Lasso Regression (L1 Regularization)** to automatically select only the most useful ones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# First, let's bring in the tools we need.\n",
                "# Pandas and numpy for data wrangling, and sklearn for the machine learning pipeline.\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.linear_model import LassoCV\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Data Loading\n",
                "We have our dataset split into training and testing files. It's crucial to evaluate our final model on the testing set to make sure it generalizes well to unseen data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's load up the CSV files into pandas DataFrames.\n",
                "train_df = pd.read_csv(\"life_expectancy_train_master.csv\")\n",
                "test_df = pd.read_csv(\"life_expectancy_test_master.csv\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Feature Selection\n",
                "We want to predict `LifeExpectancy`. Everything else in the dataset can be used as a predictor (feature). So we separate our target variable (`y`) from our feature matrix (`X`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define what we are trying to predict\n",
                "target = \"LifeExpectancy\"\n",
                "\n",
                "# Split the training data into features (X) and target (y)\n",
                "y_train = train_df[target]\n",
                "X_train = train_df.drop(columns=[target])\n",
                "\n",
                "# Do the exact same for the testing data\n",
                "y_test = test_df[target]\n",
                "X_test = test_df.drop(columns=[target])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Model Training & Evaluation\n",
                "Here is where the magic happens:\n",
                "1.  **Polynomial Features**: We use `degree=2` to create squared terms (e.g., $BMI^2$) and interaction terms (e.g., $BMI \times GDP$) for all our features. This explodes our feature count from ~27 to over 400!\n",
                "2.  **LassoCV**: Since 400+ features will definitely cause overfitting, we use Lasso Regression. The `CV` stands for Cross-Validation, meaning the model will test a bunch of different regularization strengths (`alphas`) and pick the best one for us. Lasso is great because it sets the coefficients of useless features exactly to 0, acting as a built-in feature selector."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best alpha found by LassoCV: 31.0359\n",
                        "Number of features kept: 23 out of 405\n",
                        "\n",
                        "--- Training Performance ---\n",
                        "R^2:  0.8240\n",
                        "RMSE: 4.0092\n",
                        "\n",
                        "--- Testing Performance ---\n",
                        "R^2:  0.8263\n",
                        "RMSE: 3.8805\n",
                        "\n",
                        "--- Features kept by Lasso ---\n",
                        "- Year^2\n",
                        "- Year AdultMortality\n",
                        "- Year InfantMortality\n",
                        "- Year Alcohol\n",
                        "- Year percentage expenditure\n",
                        "- Year Hepatitis B\n",
                        "- Year Measles\n",
                        "- Year BMI\n",
                        "- Year under-five deaths\n",
                        "- Year Polio\n",
                        "- Year HealthExpenditure\n",
                        "- Year Diphtheria\n",
                        "- Year HIV_AIDS\n",
                        "- Year Population\n",
                        "- Year thinness  1-19 years\n",
                        "- Year IncomeLevel\n",
                        "- Year Schooling\n",
                        "- Year Status_Encoded\n",
                        "- Year health_efficiency_ratio\n",
                        "- Year health_gdp_ratio\n",
                        "- Year log_GDP\n",
                        "- Year mortality_health_interaction\n",
                        "- Year education_income_interaction\n"
                    ]
                }
            ],
            "source": [
                "# Generate polynomial and interaction features\n",
                "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
                "X_train_poly = poly.fit_transform(X_train)\n",
                "X_test_poly = poly.transform(X_test) # Crucial: only use .transform() on test data to prevent data leakage!\n",
                "\n",
                "# Set up the Lasso model with 5-fold cross-validation to find the best alpha\n",
                "model = LassoCV(cv=5, random_state=42, n_alphas=50, max_iter=10000)\n",
                "model.fit(X_train_poly, y_train)\n",
                "\n",
                "# Let's see what the model figured out\n",
                "print(f\"Best alpha found by LassoCV: {model.alpha_:.4f}\")\n",
                "print(f\"Number of features kept: {np.sum(model.coef_ != 0)} out of {X_train_poly.shape[1]}\")\n",
                "\n",
                "# --- Evaluation ---\n",
                "\n",
                "# How well does it fit the data it was trained on?\n",
                "y_train_pred = model.predict(X_train_poly)\n",
                "print(\"\\n--- Training Performance ---\")\n",
                "print(f\"R^2:  {r2_score(y_train, y_train_pred):.4f}\")\n",
                "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
                "\n",
                "# How well does it generalize to new data? (This is the real test)\n",
                "y_test_pred = model.predict(X_test_poly)\n",
                "print(\"\\n--- Testing Performance ---\")\n",
                "print(f\"R^2:  {r2_score(y_test, y_test_pred):.4f}\")\n",
                "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
                "\n",
                "# Finally, let's look at which features were actually useful enough to keep\n",
                "print(\"\\n--- Features kept by Lasso ---\")\n",
                "feature_names = poly.get_feature_names_out(X_train.columns)\n",
                "kept_features = feature_names[model.coef_ != 0]\n",
                "for feature in kept_features:\n",
                "    print(f\"- {feature}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
